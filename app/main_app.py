import os
import asyncio
from dotenv import load_dotenv
from pathlib import Path

# –ò–º–ø–æ—Ä—Ç—ã –º–æ–¥—É–ª–µ–π
from app.llm.openai import OpenAILLM
from app.llm.llamacpp import LlamaCppLLM, SaigaLlamaCppLLM, MistralLlamaCppLLM
from app.vectorstore.faiss import FAISSVectorStore
from app.rag.ingest import DocumentIngestor
from app.rag.retriever import Retriever
from app.rag.generator import Generator
from app.rag.pipeline import RAGPipeline
from app.api.telegram import TelegramBot


async def initialize_vectorstore():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
    vectorstore = FAISSVectorStore()
    
    # –ü—É—Ç—å –∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É —Ö—Ä–∞–Ω–∏–ª–∏—â—É
    vector_store_path = os.getenv("VECTOR_STORE_PATH", "./data/vectorstore")
    
    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    if os.path.exists(f"{vector_store_path}.index"):
        print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞...")
        await vectorstore.load(vector_store_path)
        print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {vectorstore.index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")
    else:
        print("üÜï –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –µ—Å–ª–∏ –æ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        documents_path = os.getenv("DOCUMENTS_PATH", "./data/documents")
        
        if os.path.exists(documents_path):
            ingestor = DocumentIngestor(vectorstore)
            total_chunks = await ingestor.ingest_directory(
                documents_path,
                extensions=['.txt', '.md']
            )
            print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {total_chunks} —á–∞–Ω–∫–æ–≤ –∏–∑ {documents_path}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            await vectorstore.save(vector_store_path)
            print(f"‚úì –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {vector_store_path}")
        else:
            print(f"‚ö† –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {documents_path}")
            print("–ë–æ—Ç –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
    
    return vectorstore


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    load_dotenv()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
    telegram_token = os.getenv("TELEGRAM_BOT_TOKEN")
    openai_api_key = os.getenv("OPENAI_API_KEY")
    
    if not telegram_token:
        raise ValueError("TELEGRAM_BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
    
    if not openai_api_key:
        raise ValueError("OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
    
    print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞...")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    print("ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM...")
    
    # –í—ã–±–æ—Ä LLM: OpenAI –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
    use_local_model = os.getenv("USE_LOCAL_MODEL", "false").lower() == "true"
    
    if use_local_model:
        model_path = os.getenv("LOCAL_MODEL_PATH", "./models/saiga_llama3_8b.Q4_K_M.gguf")
        model_type = os.getenv("MODEL_TYPE", "saiga")  # saiga, mistral, llama
        
        print(f"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {model_path}")
        
        if model_type == "saiga":
            llm = SaigaLlamaCppLLM(
                model_path=model_path,
                temperature=0.7,
                n_ctx=4096,
                n_gpu_layers=0  # –£–≤–µ–ª–∏—á—å—Ç–µ –µ—Å–ª–∏ –µ—Å—Ç—å GPU
            )
        elif model_type == "mistral":
            llm = MistralLlamaCppLLM(
                model_path=model_path,
                temperature=0.7,
                n_ctx=4096,
                n_gpu_layers=0
            )
        else:
            llm = LlamaCppLLM(
                model_path=model_path,
                temperature=0.7,
                n_ctx=4096,
                n_gpu_layers=0
            )
    else:
        print("‚òÅÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è OpenAI API")
        llm = OpenAILLM(model_name="gpt-4-turbo-preview", temperature=0.7)
    
    print("üìä –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞...")
    vectorstore = await initialize_vectorstore()
    
    print("üîç –ù–∞—Å—Ç—Ä–æ–π–∫–∞ RAG pipeline...")
    retriever = Retriever(vectorstore, top_k=3)
    generator = Generator(llm)
    rag_pipeline = RAGPipeline(
        retriever=retriever,
        generator=generator,
        use_rag_threshold=0.5
    )
    
    print("üí¨ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Telegram –±–æ—Ç–∞...")
    bot = TelegramBot(token=telegram_token, rag_pipeline=rag_pipeline)
    bot.setup()
    
    print("=" * 50)
    print("‚úÖ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
    print("=" * 50)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞
    bot.run()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nüëã –ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        raise
