from typing import List, Dict, Optional
from llama_cpp import Llama
from .llm_base import BaseLLM
import os
import asyncio
import time
import threading


class LlamaCppLLM(BaseLLM):
    """LLM —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å llama.cpp –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(
        self, 
        model_path: str,
        temperature: float = 0.7,
        n_ctx: int = 4096,
        n_gpu_layers: int = 0,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ –Ω–∞ GPU (0 = —Ç–æ–ª—å–∫–æ CPU)
        n_threads: int = None,  # None = –∞–≤—Ç–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        verbose: bool = False
    ):
        super().__init__(model_path, temperature)
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Ç–æ–∫–æ–≤
        if n_threads is None:
            n_threads = os.cpu_count() or 4
        
        print(f"üîß –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_path}")
        print(f"   –ö–æ–Ω—Ç–µ–∫—Å—Ç: {n_ctx} —Ç–æ–∫–µ–Ω–æ–≤")
        print(f"   GPU —Å–ª–æ–∏: {n_gpu_layers}")
        print(f"   CPU –ø–æ—Ç–æ–∫–∏: {n_threads}")
        
        try:
            self.llm = Llama(
                model_path=model_path,
                n_ctx=n_ctx,
                n_gpu_layers=n_gpu_layers,
                n_threads=n_threads,
                verbose=verbose
            )
            print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def _create_prompt(
        self,
        message: str,
        context: Optional[List[Dict[str, str]]] = None,
        system_prompt: str = "–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É."
    ) -> str:
        """–°–æ–∑–¥–∞—Ç—å –ø—Ä–æ–º–ø—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Llama"""
        
        # –î–ª—è Llama 3 / Saiga —Ñ–æ—Ä–º–∞—Ç
        prompt = f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|>"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        if context:
            for msg in context:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                
                if role == "user":
                    prompt += f"<|start_header_id|>user<|end_header_id|>\n\n{content}<|eot_id|>"
                elif role == "assistant":
                    prompt += f"<|start_header_id|>assistant<|end_header_id|>\n\n{content}<|eot_id|>"
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        prompt += f"<|start_header_id|>user<|end_header_id|>\n\n{message}<|eot_id|>"
        prompt += "<|start_header_id|>assistant<|end_header_id|>\n\n"
        
        return prompt
    
    def _generate_sync(self, full_prompt: str, max_tokens: int) -> str:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (–¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ)"""
        thread_id = threading.current_thread().name
        print(f"üîß LLM._generate_sync: –∑–∞–ø—É—â–µ–Ω –≤ –ø–æ—Ç–æ–∫–µ {thread_id}")
        print(f"üìä LLM._generate_sync: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:")
        print(f"   - max_tokens: {max_tokens}")
        print(f"   - temperature: {self.temperature}")
        print(f"   - –¥–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞: {len(full_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")

        start_time = time.time()

        try:
            print(f"‚ö° LLM._generate_sync: –≤—ã–∑–æ–≤ self.llm()...")
            response = self.llm(
                full_prompt,
                max_tokens=max_tokens,
                temperature=self.temperature,
                top_p=0.95,
                top_k=40,
                repeat_penalty=1.1,
                stop=["<|eot_id|>", "<|end_of_text|>"],
                echo=False
            )

            elapsed = time.time() - start_time
            print(f"‚úÖ LLM._generate_sync: self.llm() –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed:.2f} —Å–µ–∫")

            answer = response['choices'][0]['text'].strip()
            print(f"üìù LLM._generate_sync: –∏–∑–≤–ª–µ—á–µ–Ω –æ—Ç–≤–µ—Ç ({len(answer)} —Å–∏–º–≤–æ–ª–æ–≤)")
            return answer

        except Exception as e:
            elapsed = time.time() - start_time
            print(f"‚ùå LLM._generate_sync: –û–®–ò–ë–ö–ê –ø–æ—Å–ª–µ {elapsed:.2f} —Å–µ–∫")
            print(f"‚ùå –¢–∏–ø –æ—à–∏–±–∫–∏: {type(e).__name__}")
            print(f"‚ùå –°–æ–æ–±—â–µ–Ω–∏–µ: {str(e)}")
            import traceback
            print(f"‚ùå Traceback:\n{traceback.format_exc()}")
            raise

    async def generate(
        self,
        prompt: str,
        context: Optional[List[Dict[str, str]]] = None,
        max_tokens: int = 1000
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞"""

        print(f"üß† LLM: —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞...")
        # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        full_prompt = self._create_prompt(prompt, context)

        print(f"üß† LLM: –ø—Ä–æ–º–ø—Ç —Å–æ–∑–¥–∞–Ω ({len(full_prompt)} —Å–∏–º–≤–æ–ª–æ–≤)")
        print(f"üß† LLM: –Ω–∞—á–∞–ª–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (max_tokens={max_tokens})...")

        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
            answer = await asyncio.to_thread(
                self._generate_sync,
                full_prompt,
                max_tokens
            )

            print(f"üß† LLM: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            print(f"üß† LLM: –æ—Ç–≤–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω ({len(answer)} —Å–∏–º–≤–æ–ª–æ–≤)")
            return answer

        except Exception as e:
            print(f"‚ùå LLM: –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"
    
    async def generate_stream(
        self,
        prompt: str,
        context: Optional[List[Dict[str, str]]] = None,
        max_tokens: int = 1000
    ):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ—Ç–æ–∫–æ–≤–æ–π –ø–µ—Ä–µ–¥–∞—á–µ–π"""
        
        full_prompt = self._create_prompt(prompt, context)
        
        try:
            stream = self.llm(
                full_prompt,
                max_tokens=max_tokens,
                temperature=self.temperature,
                top_p=0.95,
                top_k=40,
                repeat_penalty=1.1,
                stop=["<|eot_id|>", "<|end_of_text|>"],
                stream=True,
                echo=False
            )
            
            for chunk in stream:
                if chunk and 'choices' in chunk:
                    text = chunk['choices'][0].get('text', '')
                    if text:
                        yield text
                        
        except Exception as e:
            yield f"–û—à–∏–±–∫–∞: {str(e)}"


class MistralLlamaCppLLM(LlamaCppLLM):
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è Mistral –º–æ–¥–µ–ª–µ–π"""
    
    def _create_prompt(
        self,
        message: str,
        context: Optional[List[Dict[str, str]]] = None,
        system_prompt: str = "–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É."
    ) -> str:
        """–ü—Ä–æ–º–ø—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Mistral/Mixtral"""
        
        prompt = f"<s>[INST] {system_prompt}\n\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        if context:
            for msg in context:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                
                if role == "user":
                    prompt += f"{content} [/INST]"
                elif role == "assistant":
                    prompt += f"{content}</s>[INST] "
        
        # –¢–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        prompt += f"{message} [/INST]"
        
        return prompt


class SaigaLlamaCppLLM(LlamaCppLLM):
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Saiga"""
    
    def _create_prompt(
        self,
        message: str,
        context: Optional[List[Dict[str, str]]] = None,
        system_prompt: str = "–¢—ã ‚Äî –°–∞–π–≥–∞, —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –¢—ã —Ä–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞–µ—à—å —Å –ª—é–¥—å–º–∏ –∏ –ø–æ–º–æ–≥–∞–µ—à—å –∏–º."
    ) -> str:
        """–ü—Ä–æ–º–ø—Ç –¥–ª—è Saiga –º–æ–¥–µ–ª–µ–π"""
        
        # –§–æ—Ä–º–∞—Ç –¥–ª—è Saiga Llama 3
        prompt = f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|>"
        
        if context:
            for msg in context:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                
                if role == "user":
                    prompt += f"<|start_header_id|>user<|end_header_id|>\n\n{content}<|eot_id|>"
                elif role == "assistant":
                    prompt += f"<|start_header_id|>assistant<|end_header_id|>\n\n{content}<|eot_id|>"
        
        prompt += f"<|start_header_id|>user<|end_header_id|>\n\n{message}<|eot_id|>"
        prompt += "<|start_header_id|>assistant<|end_header_id|>\n\n"
        
        return prompt
